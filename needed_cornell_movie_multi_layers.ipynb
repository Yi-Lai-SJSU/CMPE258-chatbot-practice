{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "needed-cornell-movie-multi-layers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9NBK9EbTbUY",
        "colab_type": "code",
        "outputId": "4caac874-881c-4d37-ca7d-319249df5818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May  7 07:33:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRcxw8UmU7SV",
        "colab_type": "code",
        "outputId": "547c470a-8780-4459-e134-7ac05a45fb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byJepRuKU994",
        "colab_type": "code",
        "outputId": "7629e590-ed25-4069-a4f2-d6c4521abd46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFdMFg_kV65X",
        "colab_type": "code",
        "outputId": "bd92ab61-f6bf-4629-89dd-ef91be72289c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = '/content/gdrive/My Drive/Colab Notebooks/cmpe258/final-project/Version-v4/layers/seq2seq-chatbot/'\n",
        "import os\n",
        "os.chdir(path)\n",
        "!pwd"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/cmpe258/final-project/Version-v4/layers/seq2seq-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgOBkH6HWMT3",
        "colab_type": "code",
        "outputId": "b4f97629-f2d8-424e-bea2-d3d7e72720f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2.2.0rc4)\n",
            "Collecting tensorlayer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/38/649335834743e8ccc797dfc975d46af86558a37834a4fda8888544a0ee3a/tensorlayer-2.2.2-py2.py3-none-any.whl (363kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (1.18.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (4.38.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (3.2.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 1)) (0.14.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.3.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.10.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.28.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (2.10.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->-r requirements.txt (line 2)) (1.12.0)\n",
            "Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (0.16.2)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (2.23.0)\n",
            "Collecting imageio>=2.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/2b/9dd19644f871b10f7e32eb2dbd6b45149c350b4d5f2893e091b882e03ab7/imageio-2.8.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 8.7MB/s \n",
            "\u001b[?25hCollecting progressbar2>=3.39.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/37/b6ec42c1c8521c083f21d5f5118ca4ffc438d4ad9dd8367638674ea0febd/progressbar2-3.51.3-py2.py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from tensorlayer->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow->-r requirements.txt (line 2)) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.4)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21.0->tensorlayer->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2>=3.39.3->tensorlayer->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->tensorlayer->-r requirements.txt (line 3)) (2.8.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow->-r requirements.txt (line 2)) (0.4.8)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: imageio, progressbar2, tensorlayer\n",
            "  Found existing installation: imageio 2.4.1\n",
            "    Uninstalling imageio-2.4.1:\n",
            "      Successfully uninstalled imageio-2.4.1\n",
            "  Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "Successfully installed imageio-2.8.0 progressbar2-3.51.3 tensorlayer-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPv-LM1uab6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorlayer as tl\n",
        "import numpy as np\n",
        "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
        "from tqdm import tqdm\n",
        "from sklearn.utils import shuffle\n",
        "# from data.twitter import data\n",
        "from tensorlayer.models.seq2seq import Seq2seq\n",
        "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBGMLoMDmlrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EN_WHITELIST = '0123456789abcdefghijklmnopqrstuvwxyz ' # space is included in whitelist\n",
        "EN_BLACKLIST = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~\\''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgT2yLkRmqZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "limit = {\n",
        "        'maxq' : 25,\n",
        "        'minq' : 2,\n",
        "        'maxa' : 25,\n",
        "        'mina' : 2\n",
        "        }\n",
        "\n",
        "UNK = 'unk'\n",
        "VOCAB_SIZE = 8000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONe5u8IomsTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdhthUX0m7pg",
        "colab_type": "code",
        "outputId": "4ad6fe97-3923-4a7d-ec66-a0ce2ddb4e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/cmpe258/final-project/Version-v4/layers/seq2seq-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MEVZBhJoJgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_id2line():\n",
        "    lines=open('data/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    id2line = {}\n",
        "    for line in lines:\n",
        "        _line = line.split(' +++$+++ ')\n",
        "        if len(_line) == 5:\n",
        "            id2line[_line[0]] = _line[4]\n",
        "    return id2line\n",
        "\n",
        "def get_conversations():\n",
        "    conv_lines = open('data/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "    convs = [ ]\n",
        "    for line in conv_lines[:-1]:\n",
        "        _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        convs.append(_line.split(','))\n",
        "    return convs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d3x9bE4oonG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_conversations(convs,id2line,path='data'):\n",
        "    idx = 0\n",
        "    for conv in convs:\n",
        "        f_conv = open(path + str(idx)+'.txt', 'w')\n",
        "        for line_id in conv:\n",
        "            f_conv.write(id2line[line_id])\n",
        "            f_conv.write('\\n')\n",
        "        f_conv.close()\n",
        "        idx += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDYoli2NorEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gather_dataset(convs, id2line):\n",
        "    questions = []; answers = []\n",
        "\n",
        "    for conv in convs:\n",
        "        if len(conv) %2 != 0:\n",
        "            conv = conv[:-1]\n",
        "        for i in range(len(conv)):\n",
        "            if i%2 == 0:\n",
        "                questions.append(id2line[conv[i]])\n",
        "            else:\n",
        "                answers.append(id2line[conv[i]])\n",
        "\n",
        "    return questions, answers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3wDoWY-o5wV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_seq2seq_files(questions, answers, path='data/',TESTSET_SIZE = 30000):\n",
        "\n",
        "    # open files\n",
        "    train_enc = open(path + 'train.enc','w')\n",
        "    train_dec = open(path + 'train.dec','w')\n",
        "    test_enc  = open(path + 'test.enc', 'w')\n",
        "    test_dec  = open(path + 'test.dec', 'w')\n",
        "\n",
        "    # choose 30,000 (TESTSET_SIZE) items to put into testset\n",
        "    test_ids = random.sample([i for i in range(len(questions))],TESTSET_SIZE)\n",
        "\n",
        "    for i in range(len(questions)):\n",
        "        if i in test_ids:\n",
        "            test_enc.write(questions[i]+'\\n')\n",
        "            test_dec.write(answers[i]+ '\\n' )\n",
        "        else:\n",
        "            train_enc.write(questions[i]+'\\n')\n",
        "            train_dec.write(answers[i]+ '\\n' )\n",
        "        if i%10000 == 0:\n",
        "            print('\\n>> written {} lines'.format(i))\n",
        "\n",
        "    # close files\n",
        "    train_enc.close()\n",
        "    train_dec.close()\n",
        "    test_enc.close()\n",
        "    test_dec.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02L-q0T_osDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_line(line, whitelist):\n",
        "    return ''.join([ ch for ch in line if ch in whitelist ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lddai-BfpEal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_data(qseq, aseq):\n",
        "    filtered_q, filtered_a = [], []\n",
        "    raw_data_len = len(qseq)\n",
        "\n",
        "    assert len(qseq) == len(aseq)\n",
        "\n",
        "    for i in range(raw_data_len):\n",
        "        qlen, alen = len(qseq[i].split(' ')), len(aseq[i].split(' '))\n",
        "        if qlen >= limit['minq'] and qlen <= limit['maxq']:\n",
        "            if alen >= limit['mina'] and alen <= limit['maxa']:\n",
        "                filtered_q.append(qseq[i])\n",
        "                filtered_a.append(aseq[i])\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((raw_data_len - filt_data_len)*100/raw_data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12djeixdpHJA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def index_(tokenized_sentences, vocab_size):\n",
        "    # get frequency distribution\n",
        "    freq_dist = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "    # get vocabulary of 'vocab_size' most used words\n",
        "    vocab = freq_dist.most_common(vocab_size)\n",
        "    # index2word\n",
        "    index2word = ['_'] + [UNK] + [ x[0] for x in vocab ]\n",
        "    # word2index\n",
        "    word2index = dict([(w,i) for i,w in enumerate(index2word)] )\n",
        "    return index2word, word2index, freq_dist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jrREd7tpQHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_unk(qtokenized, atokenized, w2idx):\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    filtered_q, filtered_a = [], []\n",
        "\n",
        "    for qline, aline in zip(qtokenized, atokenized):\n",
        "        unk_count_q = len([ w for w in qline if w not in w2idx ])\n",
        "        unk_count_a = len([ w for w in aline if w not in w2idx ])\n",
        "        if unk_count_a <= 2:\n",
        "            if unk_count_q > 0:\n",
        "                if unk_count_q/len(qline) > 0.2:\n",
        "                    pass\n",
        "            filtered_q.append(qline)\n",
        "            filtered_a.append(aline)\n",
        "\n",
        "    # print the fraction of the original data, filtered\n",
        "    filt_data_len = len(filtered_q)\n",
        "    filtered = int((data_len - filt_data_len)*100/data_len)\n",
        "    print(str(filtered) + '% filtered from original data')\n",
        "\n",
        "    return filtered_q, filtered_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Pj8V49pRXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def zero_pad(qtokenized, atokenized, w2idx):\n",
        "    # num of rows\n",
        "    data_len = len(qtokenized)\n",
        "\n",
        "    # numpy arrays to store indices\n",
        "    idx_q = np.zeros([data_len, limit['maxq']], dtype=np.int32)\n",
        "    idx_a = np.zeros([data_len, limit['maxa']], dtype=np.int32)\n",
        "\n",
        "    for i in range(data_len):\n",
        "        q_indices = pad_seq(qtokenized[i], w2idx, limit['maxq'])\n",
        "        a_indices = pad_seq(atokenized[i], w2idx, limit['maxa'])\n",
        "\n",
        "        #print(len(idx_q[i]), len(q_indices))\n",
        "        #print(len(idx_a[i]), len(a_indices))\n",
        "        idx_q[i] = np.array(q_indices)\n",
        "        idx_a[i] = np.array(a_indices)\n",
        "\n",
        "    return idx_q, idx_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEZwEehRpV_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_seq(seq, lookup, maxlen):\n",
        "    indices = []\n",
        "    for word in seq:\n",
        "        if word in lookup:\n",
        "            indices.append(lookup[word])\n",
        "        else:\n",
        "            indices.append(lookup[UNK])\n",
        "    return indices + [0]*(maxlen - len(seq))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXCrX_etpYvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data():\n",
        "    id2line = get_id2line()\n",
        "    print('>> gathered id2line dictionary.\\n')\n",
        "    convs = get_conversations()\n",
        "    print(convs[121:125])\n",
        "    print('>> gathered conversations.\\n')\n",
        "    questions, answers = gather_dataset(convs,id2line)\n",
        "\n",
        "    # change to lower case (just for en)\n",
        "    questions = [ line.lower() for line in questions ]\n",
        "    answers = [ line.lower() for line in answers ]\n",
        "\n",
        "    # filter out unnecessary characters\n",
        "    print('\\n>> Filter lines')\n",
        "    questions = [ filter_line(line, EN_WHITELIST) for line in questions ]\n",
        "    answers = [ filter_line(line, EN_WHITELIST) for line in answers ]\n",
        "\n",
        "    # filter out too long or too short sequences\n",
        "    print('\\n>> 2nd layer of filtering')\n",
        "    qlines, alines = filter_data(questions, answers)\n",
        "\n",
        "    for q,a in zip(qlines[141:145], alines[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # convert list of [lines of text] into list of [list of words ]\n",
        "    print('\\n>> Segment lines into words')\n",
        "    qtokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in qlines ]\n",
        "    atokenized = [ [w.strip() for w in wordlist.split(' ') if w] for wordlist in alines ]\n",
        "    print('\\n:: Sample from segmented list of words')\n",
        "\n",
        "    for q,a in zip(qtokenized[141:145], atokenized[141:145]):\n",
        "        print('q : [{0}]; a : [{1}]'.format(q,a))\n",
        "\n",
        "    # indexing -> idx2w, w2idx\n",
        "    print('\\n >> Index words')\n",
        "    idx2w, w2idx, freq_dist = index_( qtokenized + atokenized, vocab_size=VOCAB_SIZE)\n",
        "\n",
        "    # filter out sentences with too many unknowns\n",
        "    print('\\n >> Filter Unknowns')\n",
        "    qtokenized, atokenized = filter_unk(qtokenized, atokenized, w2idx)\n",
        "    print('\\n Final dataset len : ' + str(len(qtokenized)))\n",
        "\n",
        "\n",
        "    print('\\n >> Zero Padding')\n",
        "    idx_q, idx_a = zero_pad(qtokenized, atokenized, w2idx)\n",
        "\n",
        "    print('\\n >> Save numpy arrays to disk')\n",
        "    # save them\n",
        "    np.save('data/idx_q.npy', idx_q)\n",
        "    np.save('data/idx_a.npy', idx_a)\n",
        "\n",
        "    # let us now save the necessary dictionaries\n",
        "    metadata = {\n",
        "            'w2idx' : w2idx,\n",
        "            'idx2w' : idx2w,\n",
        "            'limit' : limit,\n",
        "            'freq_dist' : freq_dist}\n",
        "\n",
        "    # write to disk : data control dictionaries\n",
        "    with open('data/metadata.pkl', 'wb') as f:\n",
        "        pickle.dump(metadata, f)\n",
        "\n",
        "    # count of unknowns\n",
        "    unk_count = (idx_q == 1).sum() + (idx_a == 1).sum()\n",
        "    # count of words\n",
        "    word_count = (idx_q > 1).sum() + (idx_a > 1).sum()\n",
        "\n",
        "    print('% unknown : {0}'.format(100 * (unk_count/word_count)))\n",
        "    print('Dataset count : ' + str(idx_q.shape[0]))\n",
        "\n",
        "\n",
        "    #print '>> gathered questions and answers.\\n'\n",
        "    #prepare_seq2seq_files(questions,answers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfFu5SVopgch",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from random import sample\n",
        "def split_dataset(x, y, ratio = [0.9, 0.05, 0.05] ):\n",
        "    # number of examples\n",
        "    data_len = len(x)\n",
        "    lens = [ int(data_len*item) for item in ratio ]\n",
        "\n",
        "    trainX, trainY = x[:lens[0]], y[:lens[0]]\n",
        "    testX, testY = x[lens[0]:lens[0]+lens[1]], y[lens[0]:lens[0]+lens[1]]\n",
        "    validX, validY = x[-lens[-1]:], y[-lens[-1]:]\n",
        "\n",
        "    return (trainX,trainY), (testX,testY), (validX,validY)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwSQfKBhphrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_gen(x, y, batch_size):\n",
        "    # infinite while\n",
        "    while True:\n",
        "        for i in range(0, len(x), batch_size):\n",
        "            if (i+1)*batch_size < len(x):\n",
        "                yield x[i : (i+1)*batch_size ].T, y[i : (i+1)*batch_size ].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ0B921_pk7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rand_batch_gen(x, y, batch_size):\n",
        "    while True:\n",
        "        sample_idx = sample(list(np.arange(len(x))), batch_size)\n",
        "        yield x[sample_idx].T, y[sample_idx].T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNxDEDfgpqcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode(sequence, lookup, separator=''): # 0 used for padding, is ignored\n",
        "    return separator.join([ lookup[element] for element in sequence if element ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZEZGNaezp1l",
        "colab_type": "code",
        "outputId": "b08b1aaa-f68d-4001-bb16-40799c9f7f75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pwd"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/cmpe258/final-project/Version-v4/layers/seq2seq-chatbot\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P62dr-wp6oA",
        "colab_type": "code",
        "outputId": "9e86148f-37ff-4575-e38c-a6100a0794f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "process_data()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> gathered id2line dictionary.\n",
            "\n",
            "[['L447', 'L448'], ['L490', 'L491'], ['L716', 'L717', 'L718', 'L719', 'L720', 'L721'], ['L750', 'L751', 'L752', 'L753', 'L754', 'L755']]\n",
            ">> gathered conversations.\n",
            "\n",
            "\n",
            ">> Filter lines\n",
            "\n",
            ">> 2nd layer of filtering\n",
            "28% filtered from original data\n",
            "q : [you hate me dont you]; a : [i dont really think you warrant that strong an emotion]\n",
            "q : [then say youll spend dollar night at the track with me]; a : [and why would i do that]\n",
            "q : [come on  the ponies the flat beer you with money in your eyes me with my hand on your ass]; a : [you  covered in my vomit]\n",
            "q : [are you following me]; a : [i was in the laundromat i saw your car thought id say hi]\n",
            "\n",
            ">> Segment lines into words\n",
            "\n",
            ":: Sample from segmented list of words\n",
            "q : [['you', 'hate', 'me', 'dont', 'you']]; a : [['i', 'dont', 'really', 'think', 'you', 'warrant', 'that', 'strong', 'an', 'emotion']]\n",
            "q : [['then', 'say', 'youll', 'spend', 'dollar', 'night', 'at', 'the', 'track', 'with', 'me']]; a : [['and', 'why', 'would', 'i', 'do', 'that']]\n",
            "q : [['come', 'on', 'the', 'ponies', 'the', 'flat', 'beer', 'you', 'with', 'money', 'in', 'your', 'eyes', 'me', 'with', 'my', 'hand', 'on', 'your', 'ass']]; a : [['you', 'covered', 'in', 'my', 'vomit']]\n",
            "q : [['are', 'you', 'following', 'me']]; a : [['i', 'was', 'in', 'the', 'laundromat', 'i', 'saw', 'your', 'car', 'thought', 'id', 'say', 'hi']]\n",
            "\n",
            " >> Index words\n",
            "\n",
            " >> Filter Unknowns\n",
            "2% filtered from original data\n",
            "\n",
            " Final dataset len : 96473\n",
            "\n",
            " >> Zero Padding\n",
            "\n",
            " >> Save numpy arrays to disk\n",
            "% unknown : 4.27073242918805\n",
            "Dataset count : 96473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7utdTyhVi243",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data(PATH='data/cornell_corpus'):\n",
        "    # read data control dictionaries\n",
        "    with open(PATH + 'metadata.pkl', 'rb') as f:\n",
        "        metadata = pickle.load(f)\n",
        "    # read numpy arrays\n",
        "    idx_q = np.load(PATH + 'idx_q.npy')\n",
        "    idx_a = np.load(PATH + 'idx_a.npy')\n",
        "    return metadata, idx_q, idx_a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLjGIt6dbUa7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_corpus = \"cornell_corpus\"\n",
        "def initial_setup(data_corpus):\n",
        "    metadata, idx_q, idx_a = load_data(PATH='data/{}/'.format(data_corpus))\n",
        "    (trainX, trainY), (testX, testY), (validX, validY) = split_dataset(idx_q, idx_a)\n",
        "    trainX = tl.prepro.remove_pad_sequences(trainX.tolist())\n",
        "    trainY = tl.prepro.remove_pad_sequences(trainY.tolist())\n",
        "    testX = tl.prepro.remove_pad_sequences(testX.tolist())\n",
        "    testY = tl.prepro.remove_pad_sequences(testY.tolist())\n",
        "    validX = tl.prepro.remove_pad_sequences(validX.tolist())\n",
        "    validY = tl.prepro.remove_pad_sequences(validY.tolist())\n",
        "    return metadata, trainX, trainY, testX, testY, validX, validY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hdym26ZbeP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata, trainX, trainY, testX, testY, validX, validY = initial_setup(data_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cPzRmFbjzH",
        "colab_type": "code",
        "outputId": "62d21aae-1ce4-43ff-bd17-d40c2e1095dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "src_len = len(trainX)\n",
        "print(src_len)\n",
        "tgt_len = len(trainY)\n",
        "print(tgt_len)\n",
        "assert src_len == tgt_len"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86825\n",
            "86825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7UpsgRbxa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "n_step = src_len // batch_size\n",
        "src_vocab_size = len(metadata['idx2w']) \n",
        "emb_dim = 1024\n",
        "\n",
        "word2idx = metadata['w2idx'] \n",
        "idx2word = metadata['idx2w']\n",
        "\n",
        "unk_id = word2idx['unk']   \n",
        "pad_id = word2idx['_']\n",
        "\n",
        "start_id = src_vocab_size  \n",
        "end_id = src_vocab_size + 1\n",
        "\n",
        "word2idx.update({'start_id': start_id})\n",
        "word2idx.update({'end_id': end_id})\n",
        "idx2word = idx2word + ['start_id', 'end_id']\n",
        "\n",
        "src_vocab_size = tgt_vocab_size = src_vocab_size + 2\n",
        "\n",
        "num_epochs = 50\n",
        "vocabulary_size = src_vocab_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ReTuSsznb2GT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inference(seed, top_n):\n",
        "  model_.eval()\n",
        "  seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
        "  sentence_id = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
        "  sentence = []\n",
        "  for w_id in sentence_id[0]:\n",
        "      w = idx2word[w_id]\n",
        "      if w == 'end_id':\n",
        "          break\n",
        "      sentence = sentence + [w]\n",
        "  return sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gGhL-8vb7hY",
        "colab_type": "code",
        "outputId": "555c4694-2ebd-433f-d506-6d0a5b0e5607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "decoder_seq_length = 20\n",
        "model_ = Seq2seq(\n",
        "    decoder_seq_length = decoder_seq_length,\n",
        "    cell_enc=tf.keras.layers.GRUCell,\n",
        "    cell_dec=tf.keras.layers.GRUCell,\n",
        "    n_layer=3,\n",
        "    n_units=256,\n",
        "    embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TL] Embedding embedding_1: (8004, 1024)\n",
            "[TL] RNN rnn_1: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_2: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_3: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_4: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_5: cell: GRUCell, n_units: 256\n",
            "[TL] RNN rnn_6: cell: GRUCell, n_units: 256\n",
            "[TL] Reshape reshape_1\n",
            "[TL] Dense  dense_1: 8004 No Activation\n",
            "[TL] Reshape reshape_2\n",
            "[TL] Reshape reshape_3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dhq5jhjWb_Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "model_.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp5ROrz9cCy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Uncomment below statements if you have already saved the model\n",
        "# load_weights = tl.files.load_npz(name='model.npz')\n",
        "# tl.files.assign_weights(load_weights, model_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqXERdh3Z7c6",
        "colab_type": "code",
        "outputId": "919fbf49-1a16-4eb9-e9d2-2942071f33d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "seeds = [\"happy birthday have a nice day\", \n",
        "         \"how are you\", \n",
        "         \"I want to have some coffee\", \n",
        "         \"how old are you\", \n",
        "         \"what do you think about coffee\",\n",
        "         \"do you like this school\",\n",
        "         \"do you want to go shopping\"]\n",
        "for epoch in range(num_epochs):\n",
        "    model_.train()\n",
        "    trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
        "    total_loss, n_iter = 0, 0\n",
        "    for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
        "                    total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
        "\n",
        "        X = tl.prepro.pad_sequences(X)\n",
        "        _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
        "        _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
        "        _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
        "        _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
        "        _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            ## compute outputs\n",
        "            output = model_(inputs = [X, _decode_seqs])\n",
        "            \n",
        "            output = tf.reshape(output, [-1, vocabulary_size])\n",
        "            ## compute loss and update model\n",
        "            loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
        "\n",
        "            grad = tape.gradient(loss, model_.all_weights)\n",
        "            optimizer.apply_gradients(zip(grad, model_.all_weights))\n",
        "        \n",
        "        total_loss += loss\n",
        "        n_iter += 1\n",
        "\n",
        "    # printing average loss after every epoch\n",
        "    print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(\"Query >\", seed)\n",
        "        top_n = 3\n",
        "        for i in range(top_n):\n",
        "            sentence = inference(seed, top_n)\n",
        "            print(\" >\", ' '.join(sentence))\n",
        "    model_path = 'model-cornell-{}.npz'.format(epoch)\n",
        "    tl.files.save_npz(model_.all_weights, name=model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/50]: loss 5.2160\n",
            "Query > happy birthday have a nice day\n",
            " > i dont want you to see me\n",
            " > i know you were in the same\n",
            " > i know you were not unk and unk unk\n",
            "Query > how are you\n",
            " > i just dont want you\n",
            " > i just dont know what i am was going\n",
            " > i know i know what to do i know what to tell you\n",
            "Query > I want to have some coffee\n",
            " > i dont know what i am\n",
            " > i dont think i was in the unk\n",
            " > i know what i mean is so much i dont want you to do\n",
            "Query > how old are you\n",
            " > i know i dont know\n",
            " > i just want to be in the world in a world\n",
            " > i dont want you know it is i just want to do\n",
            "Query > what do you think about coffee\n",
            " > i didnt say i know i dont want a favor\n",
            " > i know you dont want to get a unk in my unk unk unk and\n",
            " > i didnt want to see\n",
            "Query > do you like this school\n",
            " > i dont know i know\n",
            " > i didnt\n",
            " > i dont want it\n",
            "Query > do you want to go shopping\n",
            " > i know you were unk\n",
            " > i dont want you in the car\n",
            " > i know i dont want you\n",
            "[TL] [*] Saving TL weights into model-cornell-0.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [2/50]: loss 4.6162\n",
            "Query > happy birthday have a nice day\n",
            " > i think so\n",
            " > i think i was unk in a unk unk\n",
            " > i think so i didnt know\n",
            "Query > how are you\n",
            " > i think i was a very great man in the unk\n",
            " > i think so i dont want you back here for me for\n",
            " > i didnt do it\n",
            "Query > I want to have some coffee\n",
            " > i dont know\n",
            " > i know what you want to see me in your mouth\n",
            " > i dont want to know that i want you too much\n",
            "Query > how old are you\n",
            " > i think so\n",
            " > i didnt do anything i want\n",
            " > i think so its the unk\n",
            "Query > what do you think about coffee\n",
            " > i didnt do anything to me\n",
            " > i think so\n",
            " > i didnt know i was\n",
            "Query > do you like this school\n",
            " > i think i had to talk\n",
            " > i didnt know\n",
            " > i didnt know\n",
            "Query > do you want to go shopping\n",
            " > i didnt want a unk\n",
            " > i didnt want you\n",
            " > i dont want to talk about this stuff\n",
            "[TL] [*] Saving TL weights into model-cornell-1.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [3/50]: loss 4.3906\n",
            "Query > happy birthday have a nice day\n",
            " > i cant believe you i dont know how to feel\n",
            " > i dont know what i did to\n",
            " > i cant\n",
            "Query > how are you\n",
            " > i dont think so i dont think i could get it\n",
            " > i dont have a lot of money and\n",
            " > i just thought you were a little unk and a little bit\n",
            "Query > I want to have some coffee\n",
            " > i think you can do that for you to get a gun on the street unk\n",
            " > i dont know i dont think i had a unk unk unk but i was just very grateful\n",
            " > i cant\n",
            "Query > how old are you\n",
            " > i am\n",
            " > i am not unk\n",
            " > i was a little unk\n",
            "Query > what do you think about coffee\n",
            " > i am\n",
            " > i dont think i had it in the middle of my house i was just a little\n",
            " > i dont know what you want\n",
            "Query > do you like this school\n",
            " > no no\n",
            " > no i think\n",
            " > no im sorry\n",
            "Query > do you want to go shopping\n",
            " > no i just want a drink with a unk i cant\n",
            " > no i didnt have to\n",
            " > no i didnt mean to be\n",
            "[TL] [*] Saving TL weights into model-cornell-2.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [4/50]: loss 4.2101\n",
            "Query > happy birthday have a nice day\n",
            " > i dont want to see it was it i need to go to chicago unk and unk\n",
            " > i like it\n",
            " > i dont want to see you i know i mean it\n",
            "Query > how are you\n",
            " > i think you know i got to go home and get me a drink and get it back\n",
            " > i dont think im a good one\n",
            " > i am not sure im sure i can go\n",
            "Query > I want to have some coffee\n",
            " > i think i could\n",
            " > i cant i cant see\n",
            " > i think you should be a unk\n",
            "Query > how old are you\n",
            " > i didnt know you had a little idea\n",
            " > i dont know\n",
            " > i am\n",
            "Query > what do you think about coffee\n",
            " > i dont know what you want\n",
            " > i dont know\n",
            " > i dont know i just dont know what you want to say i am unk for unk\n",
            "Query > do you like this school\n",
            " > i think i could have been in the room and\n",
            " > i like it im just going to get it up to me i got it\n",
            " > i think we should be unk\n",
            "Query > do you want to go shopping\n",
            " > i want a unk unk to the bathroom i dont know\n",
            " > i think im gonna have to\n",
            " > i dont know what youre talking to me\n",
            "[TL] [*] Saving TL weights into model-cornell-3.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [5/50]: loss 4.0470\n",
            "Query > happy birthday have a nice day\n",
            " > i dont think i should be here\n",
            " > i think you should be unk\n",
            " > i dont know\n",
            "Query > how are you\n",
            " > i dont know i was thinking\n",
            " > i was here i didnt\n",
            " > i was just doing my unk\n",
            "Query > I want to have some coffee\n",
            " > i cant take the unk i want a beer to unk\n",
            " > i dont think so\n",
            " > i dont think so much\n",
            "Query > how old are you\n",
            " > i dont know what you mean i am not talking about\n",
            " > i dont know what i think i am im just doing that for\n",
            " > i am not unk\n",
            "Query > what do you think about coffee\n",
            " > i cant\n",
            " > i was unk the guy i dont want\n",
            " > i dont know i dont know\n",
            "Query > do you like this school\n",
            " > yeah i think i can do that\n",
            " > yeah i think i could have to get it off to her unk\n",
            " > yeah i guess\n",
            "Query > do you want to go shopping\n",
            " > i cant take you to my unk i dont know\n",
            " > i dont think i should go home and i dont\n",
            " > i want a drink\n",
            "[TL] [*] Saving TL weights into model-cornell-4.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [6/50]: loss 3.8905\n",
            "Query > happy birthday have a nice day\n",
            " > i cant imagine that\n",
            " > i dont think i could have a unk to do something\n",
            " > i cant get you any more more\n",
            "Query > how are you\n",
            " > i am\n",
            " > i am not\n",
            " > i am not\n",
            "Query > I want to have some coffee\n",
            " > no no no thanks for a minute\n",
            " > no thanks\n",
            " > no no\n",
            "Query > how old are you\n",
            " > i was hoping i had to find out\n",
            " > i dont know what i mean i dont think so but i have no right\n",
            " > i dont know\n",
            "Query > what do you think about coffee\n",
            " > i dont know where you were\n",
            " > i cant go to the hotel and tell them\n",
            " > i dont want them to be here\n",
            "Query > do you like this school\n",
            " > i think its a long trip\n",
            " > i thought you said\n",
            " > i thought i might have been in the back\n",
            "Query > do you want to go shopping\n",
            " > no no i want you\n",
            " > no sir im just getting a long long couple\n",
            " > no no no i dont want to get it\n",
            "[TL] [*] Saving TL weights into model-cornell-5.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [7/50]: loss 3.7422\n",
            "Query > happy birthday have a nice day\n",
            " > no time to get a drink\n",
            " > no problem\n",
            " > no way\n",
            "Query > how are you\n",
            " > i just want a drink to see my wife and\n",
            " > i just feel like a lot of things\n",
            " > i am im not a good woman in my unk\n",
            "Query > I want to have some coffee\n",
            " > no i dont drink\n",
            " > no problem\n",
            " > no problem\n",
            "Query > how old are you\n",
            " > i have a little surprise\n",
            " > i dont know\n",
            " > i have the keys and the unk\n",
            "Query > what do you think about coffee\n",
            " > i just want to go to the hotel but im okay\n",
            " > i cant do something\n",
            " > i cant believe you\n",
            "Query > do you like this school\n",
            " > no im fine\n",
            " > no im not going to go\n",
            " > no im fine im just fine\n",
            "Query > do you want to go shopping\n",
            " > no im okay i dont drink to the movies\n",
            " > no i do i cant i dont know\n",
            " > no im okay i just dont want to go home i just dont know where to go home and get\n",
            "[TL] [*] Saving TL weights into model-cornell-6.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [8/50]: loss 3.6038\n",
            "Query > happy birthday have a nice day\n",
            " > i dont know\n",
            " > i can take it upon myself a cab\n",
            " > i can get you a unk to be close on\n",
            "Query > how are you\n",
            " > i was in the unk i saw you in the first question my father was in a very unk place\n",
            " > i dont know i dont want to talk\n",
            " > i dont like any unk here and you know\n",
            "Query > I want to have some coffee\n",
            " > no one can get me a drink to be a problem in this place\n",
            " > no thanks im a unk\n",
            " > no thanks\n",
            "Query > how old are you\n",
            " > i dont have to say that i was in the unk\n",
            " > i am having to get you to know what you want to do is get out of this\n",
            " > i was thinking about the unk of course you should\n",
            "Query > what do you think about coffee\n",
            " > i dont want to\n",
            " > i got the idea of unk at the point\n",
            " > i dont want to hear that\n",
            "Query > do you like this school\n",
            " > yes i am unk\n",
            " > yes i do\n",
            " > yes i am unk with my mother unk\n",
            "Query > do you want to go shopping\n",
            " > no i want you to get me up with you\n",
            " > no i have to\n",
            " > no i have a long weekend i have to\n",
            "[TL] [*] Saving TL weights into model-cornell-7.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [9/50]: loss 3.4749\n",
            "Query > happy birthday have a nice day\n",
            " > excuse up\n",
            " > excuse a little boy you know who ya think about the wrong name\n",
            " > excuse a hundred bucks\n",
            "Query > how are you\n",
            " > i am\n",
            " > i was in a train\n",
            " > i was in the train and\n",
            "Query > I want to have some coffee\n",
            " > no thanks im fine im just really curious now i gotta talk about it\n",
            " > no thanks\n",
            " > no no no\n",
            "Query > how old are you\n",
            " > to do\n",
            " > to protect you for the lions bolt key box the other week and\n",
            " > to protect them\n",
            "Query > what do you think about coffee\n",
            " > i have no time\n",
            " > i have to go to work\n",
            " > i dont know\n",
            "Query > do you like this school\n",
            " > i dont think i was unk\n",
            " > i know i know i know i think its funny i mean it was all the time\n",
            " > i know\n",
            "Query > do you want to go shopping\n",
            " > no i cant do that i dont know what youre talking bout\n",
            " > no im a unk\n",
            " > no no i dont think so\n",
            "[TL] [*] Saving TL weights into model-cornell-8.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [10/50]: loss 3.3580\n",
            "Query > happy birthday have a nice day\n",
            " > did she have helped you\n",
            " > did it\n",
            " > did you see that\n",
            "Query > how are you\n",
            " > i dont have a problem\n",
            " > i dont have to explain this to me but\n",
            " > i am\n",
            "Query > I want to have some coffee\n",
            " > no thanks im a unk\n",
            " > no problem\n",
            " > no way\n",
            "Query > how old are you\n",
            " > i am with unk\n",
            " > i am dealing\n",
            " > i didnt do until scouts came here to quit\n",
            "Query > what do you think about coffee\n",
            " > i thought i had a unk on my life\n",
            " > i had to get you out of the car i got one on\n",
            " > i had a good day\n",
            "Query > do you like this school\n",
            " > no thanks im very unk\n",
            " > no thanks im unk unk unk\n",
            " > no i\n",
            "Query > do you want to go shopping\n",
            " > no thanks i just have a good time\n",
            " > no no i have to go to my car\n",
            " > no thanks i dont\n",
            "[TL] [*] Saving TL weights into model-cornell-9.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [11/50]: loss 3.2538\n",
            "Query > happy birthday have a nice day\n",
            " > oh jesus how about the kids to me\n",
            " > oh jesus\n",
            " > oh yes i did it\n",
            "Query > how are you\n",
            " > has happened\n",
            " > has happened to the city unk you know anything to say about what\n",
            " > has been told me about eve\n",
            "Query > I want to have some coffee\n",
            " > no i dont want to be in the house now\n",
            " > no i mean it isnt it i mean it aint it\n",
            " > no thanks\n",
            "Query > how old are you\n",
            " > until left unk to find them\n",
            " > until left unk to unk\n",
            " > until ran around\n",
            "Query > what do you think about coffee\n",
            " > i aint seen him\n",
            " > i dont want to see anybody\n",
            " > i thought i had something to do\n",
            "Query > do you like this school\n",
            " > yes very well thanks\n",
            " > yes i do\n",
            " > yes very good i think its too unk\n",
            "Query > do you want to go shopping\n",
            " > no i just want you\n",
            " > no no\n",
            " > no no i dont want to go\n",
            "[TL] [*] Saving TL weights into model-cornell-10.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [12/50]: loss 3.1607\n",
            "Query > happy birthday have a nice day\n",
            " > yeah its all great\n",
            " > yeah well thanks\n",
            " > yeah its great and youre gonna have to get in the kitchen tomorrow morning again with your camera for\n",
            "Query > how are you\n",
            " > has occurred to all the other things\n",
            " > has been been there i know i just know how it feels\n",
            " > has happened to decide\n",
            "Query > I want to have some coffee\n",
            " > thanks for a tour i got paid it up\n",
            " > thanks for a drink\n",
            " > thanks for unk\n",
            "Query > how old are you\n",
            " > i was twelve\n",
            " > i came in to talk about this unk now\n",
            " > i am with him\n",
            "Query > what do you think about coffee\n",
            " > i was unk the unk\n",
            " > i dont remember\n",
            " > i was just a little\n",
            "Query > do you like this school\n",
            " > yes yes yes i know i dont know i do\n",
            " > yes but its very dangerous\n",
            " > yes yes yes but\n",
            "Query > do you want to go shopping\n",
            " > no i just want to hear something about it\n",
            " > no i just want to hear it i have a unk unk\n",
            " > no but that feels good for it\n",
            "[TL] [*] Saving TL weights into model-cornell-11.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [13/50]: loss 3.0800\n",
            "Query > happy birthday have a nice day\n",
            " > i thought youd seen the first reason to be a unk\n",
            " > i thought id be back\n",
            " > i can feel a man in my way out of here for\n",
            "Query > how are you\n",
            " > has to be unk\n",
            " > has to do before you find it\n",
            " > has been unk\n",
            "Query > I want to have some coffee\n",
            " > no offense\n",
            " > no thanks\n",
            " > no thanks\n",
            "Query > how old are you\n",
            " > i didnt do i just smoke i couldnt find anywhere in the safe\n",
            " > i am dealing with the royal family\n",
            " > i am dealing with this woman\n",
            "Query > what do you think about coffee\n",
            " > i thought i had unk in a few hours i have a unk party on the floor this\n",
            " > i thought you were unk\n",
            " > i couldnt\n",
            "Query > do you like this school\n",
            " > yes i am\n",
            " > yes sir\n",
            " > yes i do but im not taking any time now i need you right\n",
            "Query > do you want to go shopping\n",
            " > no im fine thank me im not unk any time\n",
            " > no thanks ill drink it for my lunch tomorrow\n",
            " > no im just going to see it\n",
            "[TL] [*] Saving TL weights into model-cornell-12.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [14/50]: loss 3.0092\n",
            "Query > happy birthday have a nice day\n",
            " > did i ever\n",
            " > did i i have a little kiss\n",
            " > did i ever have to borrow the picture\n",
            "Query > how are you\n",
            " > eve derek\n",
            " > eve arent i unk\n",
            " > eve arent you\n",
            "Query > I want to have some coffee\n",
            " > i dont think you have it coming to\n",
            " > i have to get some work\n",
            " > i dont know i got it\n",
            "Query > how old are you\n",
            " > exactly sir\n",
            " > exactly sir and well have it easier\n",
            " > exactly were in here for the weekend\n",
            "Query > what do you think about coffee\n",
            " > i dont think so whoever couldve got it in\n",
            " > i dont know\n",
            " > i thought maybe i could get a telephone\n",
            "Query > do you like this school\n",
            " > yes sir i think its pretty mysterious however everythings going to have to motor unk to keep looking at\n",
            " > yes yes i do i love them but i like the sound on the unk side of your office in\n",
            " > yes yes i do do it all i do is make love a little while\n",
            "Query > do you want to go shopping\n",
            " > i think so\n",
            " > i dont think i could go with a drink ill do something\n",
            " > i dont want you to\n",
            "[TL] [*] Saving TL weights into model-cornell-13.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [15/50]: loss 2.9467\n",
            "Query > happy birthday have a nice day\n",
            " > i feel like very noble now so are we now here in\n",
            " > i dont want them to\n",
            " > i dont know why i dont feel like that\n",
            "Query > how are you\n",
            " > eve thought youre tired arent you\n",
            " > eve put it in my office i ever had\n",
            " > eve thought you was about red unk or eight unk of what i suppose about being\n",
            "Query > I want to have some coffee\n",
            " > no no\n",
            " > no way it was unk a friend\n",
            " > no way it is\n",
            "Query > how old are you\n",
            " > happened colonel but were fine and you know how to get in this\n",
            " > happened to ben but this meeting me\n",
            " > happened through this unk and 1 of their business in the garden\n",
            "Query > what do you think about coffee\n",
            " > i dont know i just gotta go home and go home with us\n",
            " > i just unk the surgery\n",
            " > i dont know what youre talkin about\n",
            "Query > do you like this school\n",
            " > i know its all unk address theyre just like you and i have a lot to\n",
            " > i guess\n",
            " > i think i could do this for the same rule im the unk of the unk unk\n",
            "Query > do you want to go shopping\n",
            " > no but i have to go out\n",
            " > no but you have the whole world\n",
            " > no but i dont want a unk to unk out\n",
            "[TL] [*] Saving TL weights into model-cornell-14.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [16/50]: loss 2.8928\n",
            "Query > happy birthday have a nice day\n",
            " > thank you\n",
            " > thank yuh\n",
            " > thank yuh so much\n",
            "Query > how are you\n",
            " > i must kiss\n",
            " > i must do less movement you are\n",
            " > i am i always wanted to lie down and be unk with a fool of my age thief\n",
            "Query > I want to have some coffee\n",
            " > i want to talk to annette\n",
            " > i want a look on the unk i want to be at your own ticket\n",
            " > i want a ride to unk the signal to protect you on unk\n",
            "Query > how old are you\n",
            " > i didnt have a license i was counting weeks\n",
            " > i didnt tell you but you know how you been feeling better i like it on my last\n",
            " > i didnt tell everyone was i i know i was so proud of myself im boring\n",
            "Query > what do you think about coffee\n",
            " > i didnt do it\n",
            " > i dont think i could put a hand ago\n",
            " > i thought you were a fag and you wanted to stay outside and remember\n",
            "Query > do you like this school\n",
            " > yes i do\n",
            " > yes i like you\n",
            " > yes yes you did\n",
            "Query > do you want to go shopping\n",
            " > no you dont have to\n",
            " > no no no\n",
            " > no you cant\n",
            "[TL] [*] Saving TL weights into model-cornell-15.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [17/50]: loss 2.8459\n",
            "Query > happy birthday have a nice day\n",
            " > i can stay here for the last two weeks i hope so\n",
            " > i can get a little unk on my own\n",
            " > i guess\n",
            "Query > how are you\n",
            " > i am\n",
            " > i wont\n",
            " > i tell how i was\n",
            "Query > I want to have some coffee\n",
            " > no no thanks\n",
            " > no i cant do that myself i dont know\n",
            " > no i got a date at home\n",
            "Query > how old are you\n",
            " > twenty minutes\n",
            " > twenty grand\n",
            " > twenty minutes left\n",
            "Query > what do you think about coffee\n",
            " > i didnt do it just give you a chance i dont know who that are\n",
            " > i didnt know it was a mistake\n",
            " > i cant go out with the car okay now i\n",
            "Query > do you like this school\n",
            " > yes but i really think you should be very seriously happy\n",
            " > yes but its a good time to discover my ass out of your mouth\n",
            " > yes yes yes yes\n",
            "Query > do you want to go shopping\n",
            " > i dont think i should go\n",
            " > i can go back\n",
            " > i cant go back to this church\n",
            "[TL] [*] Saving TL weights into model-cornell-16.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [18/50]: loss 2.8043\n",
            "Query > happy birthday have a nice day\n",
            " > i think so too much better though this week we can see a little more minutes tonight at night tomorrow\n",
            " > i thought i meant to\n",
            " > i think the whole thing was\n",
            "Query > how are you\n",
            " > im fine\n",
            " > im not going anywhere i could figure it out i cant\n",
            " > im tired\n",
            "Query > I want to have some coffee\n",
            " > no thanks\n",
            " > no i dont have any news\n",
            " > no thanks\n",
            "Query > how old are you\n",
            " > catch up and unk toward it\n",
            " > catch up with the first one on my unk\n",
            " > catch to me now\n",
            "Query > what do you think about coffee\n",
            " > i dont know\n",
            " > i aint gonna die\n",
            " > i dont think so\n",
            "Query > do you like this school\n",
            " > yes yes yes\n",
            " > yes yes yes i can think of that but i dont have to go home though i can get the\n",
            " > yes sir\n",
            "Query > do you want to go shopping\n",
            " > no i really am\n",
            " > no i cant hear that\n",
            " > no i cant do that\n",
            "[TL] [*] Saving TL weights into model-cornell-17.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [19/50]: loss 2.7725\n",
            "Query > happy birthday have a nice day\n",
            " > thank you annette you think we should wait til dawn i was hoping to\n",
            " > thank thank you\n",
            " > thank you thank god\n",
            "Query > how are you\n",
            " > i am\n",
            " > i am gale\n",
            " > i am\n",
            "Query > I want to have some coffee\n",
            " > no no\n",
            " > no no thanks im a little bit\n",
            " > no thank you i really dont really know what that happened\n",
            "Query > how old are you\n",
            " > twenty unk in charge to high school\n",
            " > twenty minutes\n",
            " > twenty minutes at paris is to get him out\n",
            "Query > what do you think about coffee\n",
            " > no no no\n",
            " > no i dont think so do i but not yet\n",
            " > no i dont like that unk\n",
            "Query > do you like this school\n",
            " > yup thats a lot of research\n",
            " > yup thats awful ive gotta figure out what it says very easy man thats all i think\n",
            " > yup i feel really for you to do that\n",
            "Query > do you want to go shopping\n",
            " > yeah i just need some coffee and beer tomorrow i tried to get you\n",
            " > yeah i just got to see you before you get off here for me i really really want my attention\n",
            " > yeah i really know what you want me down here ive heard all about the original movie\n",
            "[TL] [*] Saving TL weights into model-cornell-18.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [20/50]: loss 2.7387\n",
            "Query > happy birthday have a nice day\n",
            " > i wouldnt mind again\n",
            " > i wouldnt mind so happy\n",
            " > i dont know what to expect\n",
            "Query > how are you\n",
            " > i dont remember yet i think its funny but it looks like a unk im going\n",
            " > i dont smoke i dont believe you\n",
            " > i wont tell anyone about your sister\n",
            "Query > I want to have some coffee\n",
            " > no thanks\n",
            " > no no thanks unk a little late with it on the stick\n",
            " > no thanks thanks buddy i got it\n",
            "Query > how old are you\n",
            " > to do i told them i might get into them or not\n",
            " > to do i say sir im afraid its the unk\n",
            " > to do i say sir im a unk even\n",
            "Query > what do you think about coffee\n",
            " > i dont think i can get you something to do\n",
            " > i didnt do nothin about it i just wanted you and i want it for it its my birthday anymore\n",
            " > i didnt do it i mean it tastes like fish\n",
            "Query > do you like this school\n",
            " > i dont think so but if we ever unk a unk or something we can destroy it for\n",
            " > i think so although we have unk several times\n",
            " > i dont care about this stuff i dont know anything about it\n",
            "Query > do you want to go shopping\n",
            " > no its late\n",
            " > no i dont think i could go into that unk\n",
            " > no its a unk\n",
            "[TL] [*] Saving TL weights into model-cornell-19.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [21/50]: loss 2.7140\n",
            "Query > happy birthday have a nice day\n",
            " > thank you\n",
            " > thank you\n",
            " > thank you\n",
            "Query > how are you\n",
            " > i was unk\n",
            " > i am\n",
            " > i am senior i know you can trust yourself so much\n",
            "Query > I want to have some coffee\n",
            " > no no\n",
            " > no i dont\n",
            " > no no thanks im just saying\n",
            "Query > how old are you\n",
            " > second dudes the board is goin home\n",
            " > second kids\n",
            " > second ideas\n",
            "Query > what do you think about coffee\n",
            " > i think maybe we oughta know what it is till we started mac dont you\n",
            " > i cant go out with my kids and\n",
            " > i think i can swing that im sorry\n",
            "Query > do you like this school\n",
            " > i doubt that i can smell a bit pretty good to be brought\n",
            " > i doubt that im putting a little unk on us\n",
            " > i think so i think its funny\n",
            "Query > do you want to go shopping\n",
            " > no sir\n",
            " > no sir im a unk not a bad person i will have the money i shouldnt have done it on\n",
            " > no sir\n",
            "[TL] [*] Saving TL weights into model-cornell-20.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [22/50]: loss 2.6923\n",
            "Query > happy birthday have a nice day\n",
            " > thank thank you\n",
            " > thank thank ya\n",
            " > thank you\n",
            "Query > how are you\n",
            " > based of them are i on\n",
            " > based right\n",
            " > based of you whered we go\n",
            "Query > I want to have some coffee\n",
            " > no i got it on the kitchen hey yeah i gotta\n",
            " > no thanks buddy thanks i got a little unk for uh eight hours\n",
            " > no thanks\n",
            "Query > how old are you\n",
            " > to do with it\n",
            " > to find out and unk them in\n",
            " > to lose you know\n",
            "Query > what do you think about coffee\n",
            " > i dont want them unk around now\n",
            " > i dont know\n",
            " > i dont know\n",
            "Query > do you like this school\n",
            " > yup ive been in love with the future of course but i chopped off now\n",
            " > yup i think i do not know how much they want is unk unk\n",
            " > yup sure eh honey\n",
            "Query > do you want to go shopping\n",
            " > no i dont\n",
            " > no i do fine i gotta go home and go back\n",
            " > no no i didnt i\n",
            "[TL] [*] Saving TL weights into model-cornell-21.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [23/50]: loss 2.6770\n",
            "Query > happy birthday have a nice day\n",
            " > i did\n",
            " > i did not\n",
            " > i want my boy with you\n",
            "Query > how are you\n",
            " > harry please its true\n",
            " > harry and i dont know what youre saying trying to cry like this is bullshit bullshit unk\n",
            " > harry and unk unk\n",
            "Query > I want to have some coffee\n",
            " > no it aint that way\n",
            " > no thanks i dont know what that is about\n",
            " > no it wont\n",
            "Query > how old are you\n",
            " > to do charlie unk\n",
            " > to total arthur in the valley of defense to america and it will be a pleasure to us\n",
            " > to lose this energy the expert has unk in that unk with a book\n",
            "Query > what do you think about coffee\n",
            " > i dont know\n",
            " > i dont know i dont think its a good idea what i am\n",
            " > i guess you oughta drive em into basketball house right now come over\n",
            "Query > do you like this school\n",
            " > yes i can do that myself i dont think its right\n",
            " > yes i do it i have to go\n",
            " > yes i do it all the time all night in the first thing i can enjoy the good\n",
            "Query > do you want to go shopping\n",
            " > no i dont want to go to school\n",
            " > no sir im a unk\n",
            " > no sir\n",
            "[TL] [*] Saving TL weights into model-cornell-22.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [24/50]: loss 2.6599\n",
            "Query > happy birthday have a nice day\n",
            " > hey thank you\n",
            " > hey nice\n",
            " > hey man thanks nice and slow\n",
            "Query > how are you\n",
            " > not good but its true\n",
            " > not really i dont care about unk and i dont know what\n",
            " > not yet i think i have something right on the phone\n",
            "Query > I want to have some coffee\n",
            " > no problem\n",
            " > no i wont\n",
            " > no problem i dont get it out of here\n",
            "Query > how old are you\n",
            " > to charge and murder i used here\n",
            " > to have the energy waits girl to do something for me\n",
            " > to do i am here and 1\n",
            "Query > what do you think about coffee\n",
            " > i dont know what i think\n",
            " > i thought you might paste it into something\n",
            " > i thought it would be nice\n",
            "Query > do you like this school\n",
            " > yes sir\n",
            " > yes i think it would have unk us to unk me\n",
            " > yes i have no idea how i feel about that unk and very unk for now\n",
            "Query > do you want to go shopping\n",
            " > i dont think so\n",
            " > i have to\n",
            " > i think we need some help\n",
            "[TL] [*] Saving TL weights into model-cornell-23.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [25/50]: loss 2.6477\n",
            "Query > happy birthday have a nice day\n",
            " > i have a look to you and you cant\n",
            " > i got a call with my mom\n",
            " > i got that unk problem\n",
            "Query > how are you\n",
            " > i told ya i brought you back and see the address and everything\n",
            " > i must be leaving sir\n",
            " > i must have told them how do you recognize your voice\n",
            "Query > I want to have some coffee\n",
            " > no thanks i didnt do anything\n",
            " > no way\n",
            " > no thanks i dont know\n",
            "Query > how old are you\n",
            " > to beat him i am here\n",
            " > to beat him once theyre engaged\n",
            " > to breathe ma de unk\n",
            "Query > what do you think about coffee\n",
            " > i dont know i think im an unk surgeon i really am\n",
            " > i thought you were unk a few minor unk\n",
            " > i dont know i dont think im a little unk strange unk\n",
            "Query > do you like this school\n",
            " > yes but yes i am very much\n",
            " > yes i can do this for me myself but i always unk my ass\n",
            " > yes but im very proud\n",
            "Query > do you want to go shopping\n",
            " > no im fine\n",
            " > no im fine\n",
            " > no i dont\n",
            "[TL] [*] Saving TL weights into model-cornell-24.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [26/50]: loss 2.6385\n",
            "Query > happy birthday have a nice day\n",
            " > yeah really\n",
            " > yeah i guess you should come with me like the unk daughter\n",
            " > yeah i guess\n",
            "Query > how are you\n",
            " > based upon the carpet\n",
            " > based right on my mind about this i been excited you will\n",
            " > based upon building upon the heart on\n",
            "Query > I want to have some coffee\n",
            " > thanks to the bed tonight tonight\n",
            " > thanks to you buddy i dont know what i mean\n",
            " > thanks for the ride\n",
            "Query > how old are you\n",
            " > i came over here for a year and\n",
            " > i am the borg i guess\n",
            " > i came to the unk and they were dark for us\n",
            "Query > what do you think about coffee\n",
            " > i thought you were a fucking brother\n",
            " > i was just a group in a hospital here\n",
            " > i was unk\n",
            "Query > do you like this school\n",
            " > yes i know im a little boy i know you know\n",
            " > yes yes\n",
            " > yes but yes\n",
            "Query > do you want to go shopping\n",
            " > no sir\n",
            " > no i cant do it later i dont care what im going\n",
            " > no sir\n",
            "[TL] [*] Saving TL weights into model-cornell-25.npz\n",
            "[TL] [*] Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch[27/50]:  57%|█████▋    | 1550/2713 [06:26<04:54,  3.95it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0XDOMnPZ7lQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whoOhLujZ7oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}